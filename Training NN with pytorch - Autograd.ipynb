{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses in Pytorch\n",
    "Using the softmax function to predict class probabilities. With a softmax output, using cross-entropy as the loss. To actually calculate the loss, first define the criterion then pass in the output of your network and the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax gives you probabilities which will often be very close to zero or one but floating-point numbers\n",
    "#can't accurately represent values near zero or one, we use log-probabilities. \n",
    "#This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /Users/taniaincio/.pytorch/MNIST_data/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define transform to normalize the data\n",
    "# mean, std = 0.5, 0.5\n",
    "tranform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=tranform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:])\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3052, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-fordward network\n",
    "n_input = 784\n",
    "n_hiddens = [128, 64]\n",
    "n_output = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(n_input, n_hiddens[0]),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_hiddens[0], n_hiddens[1]),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_hiddens[1], n_output),\n",
    "                     nn.Softmax(dim = 1))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2865, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: it's more convenient to build the model with log-softmax output and using nn.NLLLLoss.\n",
    "# USE LogSoftmax and nn.NLLLos togheter\n",
    "# TODO:  nn.CrossEntropyLoss combines two functions nn.NLLLoss and LogSoftmax\n",
    "# it is preferable to use nn.NLLLoss since it requires output in softmax and we can easily get probabilities by using exponential\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128, 64),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(64, 10),\n",
    "                     nn.LogSoftmax(dim = 1))\n",
    "\n",
    "# TODO: Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "### Run this to check your work\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module that automatically calculating the gradients of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires_grad = True on a tensor, x.requires_grad_(True), torch.no_grad(), torch.set_grad_enabled(True|False)\n",
    "x = torch.zeros(1, requires_grad = True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4658,  1.0586],\n",
      "        [ 1.8547,  1.3425]], requires_grad=True)\n",
      "tensor([[2.1486, 1.1207],\n",
      "        [3.4399, 1.8023]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x1077b1048>\n",
      "tensor(2.1279, grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.7329,  0.5293],\n",
      "        [ 0.9274,  0.6713]])\n"
     ]
    }
   ],
   "source": [
    "# Example getting gradients with z.backward()\n",
    "x = torch.randn(2, 2, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x**2\n",
    "print(y)\n",
    "\n",
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y.mean()\n",
    "print(z)\n",
    "\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For training need the gradients of the cost with respect to the weights.\n",
    "## NOTE: With PyTorch, run data forward through the network to calculate the loss, then, go backwards to calculate the gradients with respect to the loss. \n",
    "## Once we have the gradients we can make a gradient descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Autograd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0007, 0.0007, 0.0007,  ..., 0.0007, 0.0007, 0.0007],\n",
      "        [0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0012, 0.0012, 0.0012,  ..., 0.0012, 0.0012, 0.0012],\n",
      "        [0.0012, 0.0012, 0.0012,  ..., 0.0012, 0.0012, 0.0012]])\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "n_input = 784\n",
    "n_hiddens = [128, 64]\n",
    "n_output = 10\n",
    "model = nn.Sequential(nn.Linear(n_input, n_hiddens[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_hiddens[0], n_hiddens[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_hiddens[1], n_output),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)\n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
