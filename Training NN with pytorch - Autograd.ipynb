{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses in Pytorch\n",
    "Using the softmax function to predict class probabilities. With a softmax output, using cross-entropy as the loss. To actually calculate the loss, first define the criterion then pass in the output of your network and the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax gives you probabilities which will often be very close to zero or one but floating-point numbers\n",
    "#can't accurately represent values near zero or one, we use log-probabilities. \n",
    "#This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /Users/taniaincio/.pytorch/MNIST_data/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define transform to normalize the data\n",
    "# mean, std = 0.5, 0.5\n",
    "tranform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=tranform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:])\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3052, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-fordward network\n",
    "n_input = 784\n",
    "n_hiddens = [128, 64]\n",
    "n_output = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(n_input, n_hiddens[0]),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_hiddens[0], n_hiddens[1]),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_hiddens[1], n_output),\n",
    "                     nn.Softmax(dim = 1))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2865, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: it's more convenient to build the model with log-softmax output and using nn.NLLLLoss.\n",
    "# USE LogSoftmax and nn.NLLLos togheter\n",
    "# TODO:  nn.CrossEntropyLoss combines two functions nn.NLLLoss and LogSoftmax\n",
    "# it is preferable to use nn.NLLLoss since it requires output in softmax and we can easily get probabilities by using exponential\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128, 64),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(64, 10),\n",
    "                     nn.LogSoftmax(dim = 1))\n",
    "\n",
    "# TODO: Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "### Run this to check your work\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module that automatically calculating the gradients of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires_grad = True on a tensor, x.requires_grad_(True), torch.no_grad(), torch.set_grad_enabled(True|False)\n",
    "x = torch.zeros(1, requires_grad = True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4658,  1.0586],\n",
      "        [ 1.8547,  1.3425]], requires_grad=True)\n",
      "tensor([[2.1486, 1.1207],\n",
      "        [3.4399, 1.8023]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x1077b1048>\n",
      "tensor(2.1279, grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.7329,  0.5293],\n",
      "        [ 0.9274,  0.6713]])\n"
     ]
    }
   ],
   "source": [
    "# Example getting gradients with z.backward()\n",
    "x = torch.randn(2, 2, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x**2\n",
    "print(y)\n",
    "\n",
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y.mean()\n",
    "print(z)\n",
    "\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For training need the gradients of the cost with respect to the weights.\n",
    "## NOTE: With PyTorch, run data forward through the network to calculate the loss, then, go backwards to calculate the gradients with respect to the loss. \n",
    "## Once we have the gradients we can make a gradient descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Autograd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0007, 0.0007, 0.0007,  ..., 0.0007, 0.0007, 0.0007],\n",
      "        [0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0012, 0.0012, 0.0012,  ..., 0.0012, 0.0012, 0.0012],\n",
      "        [0.0012, 0.0012, 0.0012,  ..., 0.0012, 0.0012, 0.0012]])\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "n_input = 784\n",
    "n_hiddens = [128, 64]\n",
    "n_output = 10\n",
    "model = nn.Sequential(nn.Linear(n_input, n_hiddens[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_hiddens[0], n_hiddens[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_hiddens[1], n_output),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)\n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights  Parameter containing:\n",
      "tensor([[-0.0071,  0.0002, -0.0063,  ...,  0.0177,  0.0043,  0.0170],\n",
      "        [-0.0212, -0.0194, -0.0322,  ..., -0.0105,  0.0114,  0.0264],\n",
      "        [-0.0339, -0.0109, -0.0124,  ..., -0.0131, -0.0305, -0.0015],\n",
      "        ...,\n",
      "        [ 0.0197,  0.0357, -0.0152,  ..., -0.0347,  0.0063, -0.0043],\n",
      "        [ 0.0040,  0.0053,  0.0167,  ..., -0.0202, -0.0167,  0.0079],\n",
      "        [ 0.0095, -0.0133, -0.0275,  ...,  0.0236,  0.0183,  0.0022]],\n",
      "       requires_grad=True)\n",
      "update weights  Parameter containing:\n",
      "tensor([[-0.0071,  0.0002, -0.0063,  ...,  0.0177,  0.0043,  0.0170],\n",
      "        [-0.0212, -0.0195, -0.0322,  ..., -0.0105,  0.0114,  0.0264],\n",
      "        [-0.0339, -0.0109, -0.0124,  ..., -0.0131, -0.0305, -0.0015],\n",
      "        ...,\n",
      "        [ 0.0197,  0.0357, -0.0152,  ..., -0.0347,  0.0064, -0.0043],\n",
      "        [ 0.0040,  0.0053,  0.0167,  ..., -0.0202, -0.0167,  0.0079],\n",
      "        [ 0.0096, -0.0133, -0.0275,  ...,  0.0236,  0.0183,  0.0022]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Using an optimizer to update the weights with gradients, will use Pytorch's optim\n",
    "# we can use stochastic gradient descent with optim.SGD\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "#Model\n",
    "n_input = 784\n",
    "n_hiddens = [128, 64]\n",
    "n_output = 10\n",
    "model = nn.Sequential(nn.Linear(n_input, n_hiddens[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_hiddens[0], n_hiddens[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_hiddens[1], n_output),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "\n",
    "# requiere parameters to optimize and learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "# Clear the gradients, because they are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Data\n",
    "images, label = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward Pass\n",
    "output = model(images)\n",
    "\n",
    "# Loss\n",
    "criterion = nn.NLLLoss()\n",
    "loss = criterion(output, labels)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "#print('Gradient ', model[0].weight.grad)\n",
    "\n",
    "# Update the weights\n",
    "print('initial weights ', model[0].weight)\n",
    "optimizer.step()\n",
    "print('update weights ', model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9038484609965831\n",
      "Training loss: 0.836336947993429\n",
      "Training loss: 0.5251990385464768\n",
      "Training loss: 0.4312952893502168\n",
      "Training loss: 0.3854864533903248\n"
     ]
    }
   ],
   "source": [
    "# For all data, EPOCH: one pass through the entire dataset\n",
    "n_input = 784\n",
    "n_hiddens = [128, 64]\n",
    "n_output = 10\n",
    "model = nn.Sequential(nn.Linear(n_input, n_hiddens[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_hiddens[0], n_hiddens[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_hiddens[1], n_output),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "# update weights\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.003)\n",
    "#optmizer.step()\n",
    "# loop epoch\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Predicitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "img = images[0].view(1, images[0].shape[0])\n",
    "print(img.shape)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
